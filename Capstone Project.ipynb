{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of the capstone project for data engineering is to enable students to integrate the knowledge and skills they have gained throughout the program. It is a significant component of a learner's portfolio that will contribute to achieving their career objectives in the field of data engineering. The primary objective of the project was to create an ETL pipeline for combining I94 immigration, global land temperatures, and US demographics datasets into an analytics database to analyze immigration events. The resulting analytics database can be used to identify immigration patterns to the US, such as whether people from countries with warmer or colder climates tend to immigrate to the US in greater numbers.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import utility\n",
    "import etl_functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utility)\n",
    "from utility import visualize_missing_values, clean_immigration, clean_temperature_data\n",
    "from utility import clean_demographics_data, print_formatted_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The analytics database creation process involves several steps, including:\n",
    "\n",
    "- Using Spark to load the datasets into dataframes.\n",
    "- Conducting exploratory data analysis on the I94 immigration, demographics, and global land temperatures datasets to identify missing values and develop strategies for data cleaning.\n",
    "- Performing data cleaning functions on all datasets.\n",
    "\n",
    "Creating dimension tables, including:\n",
    "- The immigration calendar dimension table, which is created from the I94 immigration dataset and linked to the fact table through the arrdate field.\n",
    "- The country dimension table, which is created from the I94 immigration and global temperatures datasets and linked to the fact table through the country of residence code, allowing analysts to explore correlations between climate and immigration to US states.\n",
    "- The USA demographics dimension table, which is created from the US cities demographics data and linked to the fact table through the state code field.\n",
    "- Creating the fact table from the cleaned I94 immigration dataset and the visa_type dimension.\n",
    "\n",
    "\n",
    "Amazon S3 and Apache Spark are the technologies used in this project. The data will be read and staged from the customer's repository using Spark. Although the project is implemented on this notebook, provisions have been made to run the ETL on a spark cluster through the etl.py script. This script reads data from S3 and creates fact and dimension tables through Spark, which are then loaded back into S3.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Immigration Data\n",
    "For many years, foreign visitors (such as business visitors, tourists, and foreign students) who entered the United States lawfully were issued the I-94 Form (Arrival/Departure Record) by U.S. immigration officers. The I-94 was a small white paper form that was given to travelers on arrival flights by cabin crews or by U.S. Customs and Border Protection at the time of entry into the United States. It contained important information such as the traveler's immigration category, port of entry, date of entry into the United States, status expiration date, and a unique 11-digit identifying number. Its primary purpose was to document the traveler's lawful admission to the United States.\n",
    "\n",
    "The I-94 dataset is the primary dataset used in this project, and it is available in the SAS binary database storage format sas7bdat. The directory *../../data/18-83510-I94-Data-2016/* contains a file for each month of 2016, totaling 12 datasets with over 40 million rows (40,790,529) and 28 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">cicid</td><td class=\"tg-0pky\">Unique record ID</td>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td>\n",
    " <tr><td class=\"tg-0pky\">i94cit</td><td class=\"tg-0pky\">3 digit code for immigrant country of birth</td>\n",
    " <tr><td class=\"tg-0pky\">i94res</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td>\n",
    " <tr><td class=\"tg-0pky\">insnum</td><td class=\"tg-0pky\">INS number</td>\n",
    " <tr><td class=\"tg-0pky\">airline</td><td class=\"tg-0pky\">Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">admnum</td><td class=\"tg-0pky\">Admission Number</td>\n",
    " <tr><td class=\"tg-0pky\">fltno</td><td class=\"tg-0pky\">Flight number of Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">visatype</td><td class=\"tg-0pky\">Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Global Land Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are various organizations that gather data on climate trends. The three most commonly referenced datasets for land and ocean temperatures are NOAA's MLOST, NASA's GISTEMP, and the UK's HadCrut.\n",
    "\n",
    "Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory, has compiled and repackaged data from a newer compilation. The Berkeley Earth Surface Temperature Study is a collection of 1.6 billion temperature reports from 16 existing archives. It is well-organized and allows for filtering of interesting subsets, such as by country. They provide both the source data and the code for the transformations applied. Additionally, they use methods that enable weather observations from shorter time series to be included, resulting in fewer observations being discarded.\n",
    "\n",
    "Although several files are available in the original dataset from Kaggle, this capstone project will only utilize the GlobalLandTemperaturesByCity dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.csv(file_name, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">dt</td><td class=\"tg-0pky\">Date</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperature</td><td class=\"tg-0pky\">Global average land temperature in celsius</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperatureUncertainty</td><td class=\"tg-0pky\">95% confidence interval around the average</td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">Name of City</td>\n",
    " <tr><td class=\"tg-0pky\">Country</td><td class=\"tg-0pky\">Name of Country</td>\n",
    " <tr><td class=\"tg-0pky\">Latitude</td><td class=\"tg-0pky\">City Latitude</td>\n",
    " <tr><td class=\"tg-0pky\">Longitude</td><td class=\"tg-0pky\">City Longitude</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The airport codes can refer to either the IATA airport code, a three-letter code used in passenger reservation, ticketing, and baggage-handling systems, or the ICAO airport code, a four-letter code used by ATC systems and for airports that do not have an IATA airport code (as per Wikipedia).\n",
    "\n",
    "The list of airport codes used around the world is available in the airport-codes.csv file. The data was downloaded from a public domain source http://ourairports.com/data/, which compiled the data from multiple different sources. The attributes in the dataset are identified in the datapackage description, and some of the columns contain attributes that identify airport locations and other codes (IATA, local if exist) that are relevant to the identification of an airport. The original source URL for this data is http://ourairports.com/data/airports.csv and is stored in the archive/data.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = \"airport-codes_csv.csv\"\n",
    "df_airport = spark.read.csv(file_name,  inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">ident</td><td class=\"tg-0pky\">Unique identifier</td>\n",
    " <tr><td class=\"tg-0pky\">type</td><td class=\"tg-0pky\">Type of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">name</td><td class=\"tg-0pky\">Airport Name</td>\n",
    " <tr><td class=\"tg-0pky\">elevation_ft</td><td class=\"tg-0pky\">Altitude of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">continent</td><td class=\"tg-0pky\">Continent</td>\n",
    " <tr><td class=\"tg-0pky\">iso_country</td><td class=\"tg-0pky\">ISO code of the country of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">iso_region</td><td class=\"tg-0pky\">ISO code for the region of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">municipality</td><td class=\"tg-0pky\">City where the airport is located</td>\n",
    " <tr><td class=\"tg-0pky\">gps_code</td><td class=\"tg-0pky\">GPS code of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">iata_code</td><td class=\"tg-0pky\">IATA code of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">local_code</td><td class=\"tg-0pky\">Local code of the airport</td>\n",
    " <tr><td class=\"tg-0pky\">coordinates</td><td class=\"tg-0pky\">GPS coordinates of the airport</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The airport dataset was not found to be a useful source for analysis in our model because we were unable to join it with the main table immigration. We could not find a valid and consistent key that could be used to link the two tables together. None of the codes in the airport dataset, such as ident, gps_code, iata_code or local_code, appeared to match the columns in the immigration fact table. Therefore, we decided not to use the airport dataset in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The dataset, obtained from OpenSoft, provides demographic information on all US cities and census-designated places with a population of 65,000 or more. The source of the data is the US Census Bureau's 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = \"us-cities-demographics.csv\"\n",
    "df_us_cities_demographics = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities_demographics.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">State Code</td><td class=\"tg-0pky\">Code of the US state</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# immigration Data\n",
    "\n",
    "utility.visualize_missing_values_spark(df_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Global temperature Data\n",
    "temperature_df = temperature_df.withColumn(\"dt\",col(\"dt\").cast(StringType())) # convert dt column type to string\n",
    "utility.visualize_missing_values_spark(temperature_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# US City Demographic Data\n",
    "utility.visualize_missing_values_spark(df_us_cities_demographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "*The height of each bar represents the percentage of missing values in that column. If a column has no missing values, the corresponding bar will be completely unfilled.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Drop columns with more than 90% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# columns with over 90% missing values\n",
    "cols = ['occup', 'entdepu','insnum']\n",
    "\n",
    "# drop these columns\n",
    "df_immigration = df_immigration.drop(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = df_immigration.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature = df_temperature.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities_demographics = df_us_cities_demographics.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "![Database schema](conceptual_model.png)\n",
    "\n",
    "The country dimension table is comprised of data from two datasets: the global land temperatures by city and the immigration datasets. This combination of data enables analysts to investigate the correlations between global land temperatures and immigration patterns to the US.\n",
    "\n",
    "The US demographics dimension table is sourced from the demographics dataset and is linked to the immigration fact table at the US state level. This dimension allows analysts to gain insights into migration patterns based on demographics and overall population of states. For example, one could explore if more visitors come to populous states on a monthly basis. By leveraging this data model, a dashboard can be created that offers granular information on visits to the US, potentially driving data-driven decision making within the tourism and immigration departments at the state level.\n",
    "\n",
    "The visa type dimension table is sourced from the immigration datasets and is linked to the immigration fact table via the visa_type_key.\n",
    "\n",
    "The immigration fact table is the centerpiece of the data model. It's comprised of data from the immigration datasets and contains keys that link to the dimension tables. Detailed information on the data that makes up the fact table can be found in the data dictionary of the immigration dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are as follows:\n",
    "\n",
    "* Load the datasets\n",
    "* Clean the I94 Immigration data to create Spark dataframe for each month\n",
    "* Create visa_type dimension table\n",
    "* Create calendar dimension table\n",
    "* Extract clean global temperatures data\n",
    "* Create country dimension table\n",
    "* Create immigration fact table\n",
    "* Load demographics data\n",
    "* Clean demographics data\n",
    "* Create demographic dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the immigration calendar dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_calendar_dimension(df, output_data):\n",
    "    \"\"\"This function creates an immigration calendar based on arrival date\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # create initial calendar df from arrdate column\n",
    "    calendar_df = df.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df.arrdate)).distinct()\n",
    "    \n",
    "    # expand df by adding other calendar columns\n",
    "    calendar_df = calendar_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_month', month('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_year', year('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "\n",
    "    # create an id field in calendar df\n",
    "    calendar_df = calendar_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write the calendar dimension to parquet file\n",
    "    partition_columns = ['arrival_year', 'arrival_month', 'arrival_week']\n",
    "    calendar_df.write.parquet(output_data + \"immigration_calendar\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "    \n",
    "    return calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar_df = create_immigration_calendar_dimension(new_immigration_df, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the country dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_country_dimension_table(df, temp_df, output_data):\n",
    "    \"\"\"This function creates a country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :temp_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get the aggregated temperature data\n",
    "    agg_temp = utility.aggregate_temperature_data(temp_df).toPandas()\n",
    "    # load the i94res to country mapping data\n",
    "    mapping_codes = pd.read_csv('i94res.csv')\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_country_average_temperature(name):\n",
    "        print(\"Processing: \", name)\n",
    "        avg_temp = agg_temp[agg_temp['Country']==name]['average_temperature']\n",
    "        \n",
    "        if not avg_temp.empty:\n",
    "            return str(avg_temp.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @udf()\n",
    "    def get_country_name(code):\n",
    "        name = mapping_codes[mapping_codes['code']==code]['Name'].iloc[0]\n",
    "        \n",
    "        if name:\n",
    "            return name.title()\n",
    "        return None\n",
    "        \n",
    "    # select and rename i94res column\n",
    "    dim_df = df.select(['i94res']).distinct() \\\n",
    "                .withColumnRenamed('i94res', 'country_code')\n",
    "    \n",
    "    # create country_name column\n",
    "    dim_df = dim_df.withColumn('country_name', get_country_name(dim_df.country_code))\n",
    "    \n",
    "    # create average_temperature column\n",
    "    dim_df = dim_df.withColumn('average_temperature', get_country_average_temperature(dim_df.country_name))\n",
    "    \n",
    "    # write the dimension to a parquet file\n",
    "    dim_df.write.parquet(output_data + \"country\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_f = create_country_dimension_table(new_immigration_df, new_temperature_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_f.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the visa type dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_visa_type_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a visa type dimension from the immigration data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # create visatype df from visatype column\n",
    "    visatype_df = df.select(['visatype']).distinct()\n",
    "    \n",
    "    # add an id column\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    visatype_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "    \n",
    "    return visatype_df\n",
    "\n",
    "def get_visa_type_dimension(output_data):\n",
    "    return spark.read.parquet(output_data + \"visatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test create visa_type dimension function\n",
    "visatype_df = create_visa_type_dimension_table(new_immigration_df, output_data)\n",
    "visatype_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the demographics dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_demographics_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a us demographics dimension table from the us cities demographics data.\n",
    "    \n",
    "    :param df: spark dataframe of us demographics survey data\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing demographics dimension\n",
    "    \"\"\"\n",
    "    dim_df = df.withColumnRenamed('Median Age','median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population') \\\n",
    "            .withColumnRenamed('Total Population', 'total_population') \\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "    # lets add an id column\n",
    "    dim_df = dim_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    dim_df.write.parquet(output_data + \"demographics\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_dim_df = create_demographics_dimension_table(new_demographics_df, output_data)\n",
    "demographics_dim_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the immigration fact table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_fact_table(df, output_data):\n",
    "    \"\"\"This function creates an country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param visa_type_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get visa_type dimension\n",
    "    dim_df = get_visa_type_dimension(output_data).toPandas()\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_visa_key(visa_type):\n",
    "        \"\"\"user defined function to get visa key\n",
    "        \n",
    "        :param visa_type: US non-immigrant visa type\n",
    "        :return: corresponding visa key\n",
    "        \"\"\"\n",
    "        key_series = dim_df[dim_df['visatype']==visa_type]['visa_type_key']\n",
    "        \n",
    "        if not key_series.empty:\n",
    "            return str(key_series.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # rename columns to align with data model\n",
    "    df = df.withColumnRenamed('cicid','record_id') \\\n",
    "            .withColumnRenamed('i94res', 'country_residence_code') \\\n",
    "            .withColumnRenamed('i94addr', 'state_code') \n",
    "    \n",
    "    # create visa_type key\n",
    "    df = df.withColumn('visa_type_key', get_visa_key('visatype'))\n",
    "    \n",
    "    # convert arrival date into datetime object\n",
    "    df = df.withColumn(\"arrdate\", get_datetime(df.arrdate))\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"immigration_fact\", mode=\"overwrite\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact_df = create_immigration_fact_table(new_immigration_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    # load data\n",
    "    \n",
    "    # run cleaning functions\n",
    "    \n",
    "    # create fact and dimension tables\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks ensures that the ETL has created fact and dimension tables with adequate records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks ensures that the ETL has created fact and dimension tables with adequate records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_dfs = {\n",
    "    'immigration_fact': immigration_fact_df,\n",
    "    'visa_type_dim': visatype_df,\n",
    "    'calendar_dim': calendar_df,\n",
    "    'usa_demographics_dim': demographics_dim_df,\n",
    "    'country_dim': country_dim_f\n",
    "}\n",
    "for table_name, table_df in table_dfs.items():\n",
    "    # quality check for table\n",
    "    etl_functions.quality_checks(table_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### Fact Table - data dictionary\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">record_id</td><td class=\"tg-0pky\">Unique record ID</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_residence_code</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td></tr>    \n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">A numerical key that links to the visa_type dimension table</td></tr>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td></tr>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td></tr>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td></tr>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td></tr>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Country Dimension Table - data dictionary\n",
    "<p>  \n",
    "<i>The country code and country_name fields come from the labels description SAS file while the average_temperature data comes from the global land temperature by cities data.</i>\n",
    "</p>\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">country_code</td><td class=\"tg-0pky\">Unique country code</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_name</td><td class=\"tg-0pky\">Name of country</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">average_temperature</td><td class=\"tg-0pky\">Average temperature of country</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Visa Type Dimension Table - data dictionary\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">Unique id for each visa issued</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type</td><td class=\"tg-0pky\">Name of visa</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### Immigration Calendar Dimension Table - data dictionary\n",
    "<p>\n",
    "<i>The whole of this dataset comes from the immigration dataset.</i>\n",
    "</p>\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Unique id</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival date into US</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">arrival_year</td><td class=\"tg-0pky\">Arrival year into US</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_month</td><td class=\"tg-0pky\">Arrival MonthS</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_day</td><td class=\"tg-0pky\">Arrival Day</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_week</td><td class=\"tg-0pky\">Arrival Week</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_weekday</td><td class=\"tg-0pky\">Arrival WeekDay</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Demographics Dimension Table - data dictionary\n",
    "<p>\n",
    "<i>The whole of this dataset comes from the us cities demographics data.</i>\n",
    "</p>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Record id</td>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state code </td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "The reason for selecting tools and technologies for the project:\n",
    "- Apache Spark was chosen because of its capability to handle large amounts of data in multiple file formats.\n",
    "- Apache Spark provides a high-speed, comprehensive analytics engine for big data.\n",
    "- Spark's APIs are user-friendly and allow for the processing of large datasets.\n",
    "\n",
    "Suggestion for the frequency of data updates and explanation:\n",
    "- The current I94 immigration data is refreshed on a monthly basis, and therefore the data will be updated monthly.\n",
    "\n",
    "Explanation of how the approach would vary under the given circumstances:\n",
    "- If the data increased by 100x, we would consider increasing the number of nodes in our cluster while Spark could manage the increase.\n",
    "- If the data populated a dashboard that required daily updates by 7am, we would schedule and execute data pipelines using Apache Airflow.\n",
    "- If the database needed to be accessed by 100+ people, we would relocate our analytics database to Amazon Redshift."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
